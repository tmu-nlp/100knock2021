{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "knock72.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "history_visible": true,
      "mount_file_id": "1X2awNedsbdhWs0wXG-X51BUXmjYQAEay",
      "authorship_tag": "ABX9TyNlArbBcONdDJtfeAZNnJDA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tmu-nlp/100knock2021/blob/main/wei/chapter08/knock72.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXT_DLp2DE4e"
      },
      "source": [
        "## Task Description\n",
        "72. 損失と勾配の計算\n",
        "\n",
        "学習データの事例$x_1$と事例集合$x_1$,$x_2$,$x_3$,$x_4$に対して，CrossEntropyLossと，行列$W$に対する勾配を計算せよ．なお，ある事例$x_i$に対して損失は次式で計算される．\n",
        "$$l_i=−log[事例x_iがy_iに分類される確率]$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_eulk7dx9T3"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# データの読込\n",
        "df = pd.read_csv('drive/MyDrive/ColabNotebooks/NLPknock100/newsCorpora_re.csv', header=None, sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n",
        "\n",
        "# データの抽出\n",
        "df = df.loc[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']), ['TITLE', 'CATEGORY']]\n",
        "\n",
        "# データの分割\n",
        "train, valid_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=123, stratify=df['CATEGORY'])\n",
        "valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, random_state=123, stratify=valid_test['CATEGORY'])"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0WTh1OhF6rV"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "import string\n",
        "import torch\n",
        "\n",
        "# 学習済み単語ベクトルを読み込む\n",
        "model = KeyedVectors.load_word2vec_format('drive/MyDrive/ColabNotebooks/NLPknock100/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "\n",
        "def transform_w2v(text):\n",
        "  table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "  words = text.translate(table).split()  # 記号をスペースに置換後、スペースで分割してリスト化\n",
        "  vec = [model[word] for word in words if word in model]  # 1語ずつベクトル化\n",
        "\n",
        "  return torch.tensor(sum(vec) / len(vec))  # 平均ベクトルをTensor型に変換して出力\n",
        "\n",
        "X_train = torch.stack([transform_w2v(text) for text in train['TITLE']])\n",
        "X_valid = torch.stack([transform_w2v(text) for text in valid['TITLE']])\n",
        "X_test = torch.stack([transform_w2v(text) for text in test['TITLE']])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUGAmqumGWT4"
      },
      "source": [
        "# ラベルベクトルの作成\n",
        "category_dict = {'b': 0, 't': 1, 'e':2, 'm':3}\n",
        "y_train = torch.tensor(train['CATEGORY'].map(lambda x: category_dict[x]).values)\n",
        "y_valid = torch.tensor(valid['CATEGORY'].map(lambda x: category_dict[x]).values)\n",
        "y_test = torch.tensor(test['CATEGORY'].map(lambda x: category_dict[x]).values)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksWFiXGiG6iU"
      },
      "source": [
        "# SGLNetという単層ニューラルネットワークを定義\n",
        "from torch import nn\n",
        "\n",
        "class SGLNet(nn.Module):\n",
        "  #　ネットのlayerを定義\n",
        "  def __init__(self, input_size, output_size):\n",
        "    super().__init__()\n",
        "    self.fc = nn.Linear(input_size, output_size, bias=False)\n",
        "    nn.init.normal_(self.fc.weight, 0.0, 1.0)  # 正規乱数で重みを初期化\n",
        "  #　forwardで入力データが順伝播時に通るレイヤーを順に配置しておく\n",
        "  def forward(self, x):\n",
        "    x = self.fc(x)\n",
        "    return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hE1VkqwmG8Dc"
      },
      "source": [
        "# 単層ニューラルネットワークの初期化\n",
        "SigelNNmodel = SGLNet(300, 4) "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4av0-invD7xj"
      },
      "source": [
        "# 学習用のTensor型の平均化ベクトルとラベルベクトルを入力することで、集合にある各事例の平均損失を計算\n",
        "# 入力ベクトルはsoftmax前の値\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "l_1 = criterion(SigelNNmodel(X_train[:1]), y_train[:1])  \n",
        "SigelNNmodel.zero_grad()  # 勾配をゼロで初期化\n",
        "l_1.backward()  # 勾配を計算\n",
        "print(f'損失: {l_1:.4f}')\n",
        "print(f'勾配:\\n{SigelNNmodel.fc.weight.grad}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEXXqCA2HkOp"
      },
      "source": [
        "#@title\n",
        "l = criterion(SigelNNmodel(X_train[:4]), y_train[:4])\n",
        "SigelNNmodel.zero_grad()\n",
        "l.backward()\n",
        "print(f'損失: {l:.4f}')\n",
        "print(f'勾配:\\n{SigelNNmodel.fc.weight.grad}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}