{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "100knock2021/chapter09.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOD+pM7QSdRoswaJARHt7Hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/u7693/4a217719a5badd1fa694925d38c23cea/100knock2021-chapter09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6oVETILkvnD"
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
        "!unzip NewsAggregatorDataset.zip\n",
        "!sed -e 's/\"/'\\''/g' ./newsCorpora.csv > ./newsCorpora_re.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkDlMhPpkwgC"
      },
      "source": [
        "## knock80"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SgROmHS3uyO"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv('./newsCorpora_re.csv', header=None, sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n",
        "df = df.loc[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']), ['TITLE', 'CATEGORY']]\n",
        "\n",
        "train, valid_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=123, stratify=df['CATEGORY'])\n",
        "valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, random_state=123, stratify=valid_test['CATEGORY'])\n",
        "train.reset_index(drop=True, inplace=True)\n",
        "valid.reset_index(drop=True, inplace=True)\n",
        "test.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rn1GzRb3wnF"
      },
      "source": [
        "from collections import defaultdict\n",
        "import string\n",
        "\n",
        "d = defaultdict(int)\n",
        "table = str.maketrans(string.punctuation, ' '*len(string.punctuation)) \n",
        "for text in train['TITLE']:\n",
        "  for word in text.translate(table).split():\n",
        "    d[word] += 1\n",
        "d = sorted(d.items(), key=lambda x:x[1], reverse=True)\n",
        "\n",
        "word2id = {word: i + 1 for i, (word, cnt) in enumerate(d) if cnt > 1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRqMMr_J3y24"
      },
      "source": [
        "import torch\n",
        "\n",
        "def tokenizer(text, word2id=word2id, unk=0):\n",
        "  table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "  return [word2id.get(word, unk) for word in text.translate(table).split()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTzTtm_kkzPs"
      },
      "source": [
        "## knock81"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx7kc3r031ba"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_size, padding_idx, output_size, hidden_size, device=None):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
        "    self.rnn = nn.RNN(emb_size, hidden_size, nonlinearity='tanh', batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "    self.rnn_device = device\n",
        "    \n",
        "  def forward(self, x):\n",
        "    self.batch_size = x.size()[0]\n",
        "    hidden = self.init_hidden() \n",
        "    emb = self.emb(x)\n",
        "    out, hidden = self.rnn(emb, hidden)\n",
        "    out = self.fc(out[:, -1, :])\n",
        "    return out\n",
        "    \n",
        "  def init_hidden(self):\n",
        "    hidden = torch.zeros(1, self.batch_size, self.hidden_size)\n",
        "    if (self.rnn_device != None):\n",
        "        hidden = hidden.to(self.rnn_device)\n",
        "    return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-HUHnHn32ZY"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CreateDataset(Dataset):\n",
        "  def __init__(self, X, y, tokenizer):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.y)\n",
        "\n",
        "  def __getitem__(self, index): \n",
        "    text = self.X[index]\n",
        "    inputs = self.tokenizer(text)\n",
        "\n",
        "    return {\n",
        "      'inputs': torch.tensor(inputs, dtype=torch.int64),\n",
        "      'labels': torch.tensor(self.y[index], dtype=torch.int64)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5bUhGWn33YF"
      },
      "source": [
        "category_dict = {'b': 0, 't': 1, 'e':2, 'm':3}\n",
        "y_train = train['CATEGORY'].map(lambda x: category_dict[x]).values\n",
        "y_valid = valid['CATEGORY'].map(lambda x: category_dict[x]).values\n",
        "y_test = test['CATEGORY'].map(lambda x: category_dict[x]).values\n",
        "\n",
        "dataset_train = CreateDataset(train['TITLE'], y_train, tokenizer)\n",
        "dataset_valid = CreateDataset(valid['TITLE'], y_valid, tokenizer)\n",
        "dataset_test = CreateDataset(test['TITLE'], y_test, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXyxG9NZ34r7"
      },
      "source": [
        "# パラメータ\n",
        "VOCAB_SIZE = len(set(word2id.values())) + 1\n",
        "EMB_SIZE = 300\n",
        "PADDING_IDX = len(set(word2id.values()))\n",
        "OUTPUT_SIZE = 4\n",
        "HIDDEN_SIZE = 50\n",
        "\n",
        "model = RNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, HIDDEN_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbBVFuzVk0qn"
      },
      "source": [
        "## knock82"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EJIK0k1359s"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "from torch import optim\n",
        "\n",
        "def calculate_loss_and_accuracy(model, dataset, device=None, criterion=None):\n",
        "  dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "  loss = 0.0\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data in dataloader:\n",
        "      inputs = data['inputs'].to(device)\n",
        "      labels = data['labels'].to(device)\n",
        "\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      if criterion != None:\n",
        "        loss += criterion(outputs, labels).item()\n",
        "\n",
        "      pred = torch.argmax(outputs, dim=-1)\n",
        "      total += len(inputs)\n",
        "      correct += (pred == labels).sum().item()\n",
        "      \n",
        "  return loss / len(dataset), correct / total\n",
        "  \n",
        "\n",
        "def train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, num_epochs, collate_fn=None, device=None):\n",
        "  model.to(device)\n",
        "\n",
        "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "  dataloader_valid = DataLoader(dataset_valid, batch_size=1, shuffle=False)\n",
        "\n",
        "  scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs, eta_min=1e-5, last_epoch=-1)\n",
        "\n",
        "  log_train = []\n",
        "  log_valid = []\n",
        "  for epoch in range(num_epochs):\n",
        "    s_time = time.time()\n",
        "\n",
        "    model.train()\n",
        "    for data in dataloader_train:\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      inputs = data['inputs'].to(device)\n",
        "      labels = data['labels'].to(device)\n",
        "      outputs = model.forward(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    loss_train, acc_train = calculate_loss_and_accuracy(model, dataset_train, device, criterion=criterion)\n",
        "    loss_valid, acc_valid = calculate_loss_and_accuracy(model, dataset_valid, device, criterion=criterion)\n",
        "    log_train.append([loss_train, acc_train])\n",
        "    log_valid.append([loss_valid, acc_valid])\n",
        "\n",
        "    torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, f'checkpoint{epoch + 1}.pt')\n",
        "\n",
        "    e_time = time.time()\n",
        "\n",
        "    print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}, {(e_time - s_time):.4f}sec') \n",
        "\n",
        "    if epoch > 2 and log_valid[epoch - 3][0] <= log_valid[epoch - 2][0] <= log_valid[epoch - 1][0] <= log_valid[epoch][0]:\n",
        "      break\n",
        "      \n",
        "    scheduler.step()\n",
        "\n",
        "  return {'train': log_train, 'valid': log_valid}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nH53yBz637HK"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def visualize_logs(log):\n",
        "  fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "  ax[0].plot(np.array(log['train']).T[0], label='train')\n",
        "  ax[0].plot(np.array(log['valid']).T[0], label='valid')\n",
        "  ax[0].set_xlabel('epoch')\n",
        "  ax[0].set_ylabel('loss')\n",
        "  ax[0].legend()\n",
        "  ax[1].plot(np.array(log['train']).T[1], label='train')\n",
        "  ax[1].plot(np.array(log['valid']).T[1], label='valid')\n",
        "  ax[1].set_xlabel('epoch')\n",
        "  ax[1].set_ylabel('accuracy')\n",
        "  ax[1].legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RySHWF9n38HP"
      },
      "source": [
        "# パラメータ\n",
        "VOCAB_SIZE = len(set(word2id.values())) + 1  \n",
        "EMB_SIZE = 300\n",
        "PADDING_IDX = len(set(word2id.values()))\n",
        "OUTPUT_SIZE = 4\n",
        "HIDDEN_SIZE = 50\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "model = RNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, HIDDEN_SIZE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "log = train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh_b7RQI39B1"
      },
      "source": [
        "visualize_logs(log)\n",
        "\n",
        "_, acc_train = calculate_loss_and_accuracy(model, dataset_train)\n",
        "_, acc_test = calculate_loss_and_accuracy(model, dataset_test)\n",
        "print(f'正解率（学習データ）：{acc_train:.3f}')\n",
        "print(f'正解率（評価データ）：{acc_test:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnEgvVIgk1QE"
      },
      "source": [
        "## knock83"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEsV0BmwsL4Y"
      },
      "source": [
        "class Padsequence():\n",
        "  def __init__(self, padding_idx):\n",
        "    self.padding_idx = padding_idx\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    sorted_batch = sorted(batch, key=lambda x: x['inputs'].shape[0], reverse=True)\n",
        "    sequences = [x['inputs'] for x in sorted_batch]\n",
        "    sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=self.padding_idx)\n",
        "    labels = torch.LongTensor([x['labels'] for x in sorted_batch])\n",
        "\n",
        "    return {'inputs': sequences_padded, 'labels': labels}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCOxazGi3_LK"
      },
      "source": [
        "# パラメータ\n",
        "VOCAB_SIZE = len(set(word2id.values())) + 1 \n",
        "EMB_SIZE = 300\n",
        "PADDING_IDX = len(set(word2id.values()))\n",
        "OUTPUT_SIZE = 4\n",
        "HIDDEN_SIZE = 50\n",
        "LEARNING_RATE = 5e-2\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "model = RNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, HIDDEN_SIZE, device=device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "log = train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, collate_fn=Padsequence(PADDING_IDX), device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPiohVVo4BAX"
      },
      "source": [
        "visualize_logs(log)\n",
        "\n",
        "_, acc_train = calculate_loss_and_accuracy(model, dataset_train, device)\n",
        "_, acc_test = calculate_loss_and_accuracy(model, dataset_test, device)\n",
        "print(f'正解率（学習データ）：{acc_train:.3f}')\n",
        "print(f'正解率（評価データ）：{acc_test:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFqo4xPfRuFs"
      },
      "source": [
        "## knock84"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxLtu8RmRwQG"
      },
      "source": [
        "FILE_ID = \"0B7XkCwpI5KDYNlNUTTlSS21pQmM\"\n",
        "FILE_NAME = \"GoogleNews-vectors-negative300.bin.gz\"\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=$FILE_ID' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=$FILE_ID\" -O $FILE_NAME && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xGnwAUJR6Yc"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "\n",
        "VOCAB_SIZE = len(set(word2id.values())) + 1\n",
        "EMB_SIZE = 300\n",
        "weights = np.zeros((VOCAB_SIZE, EMB_SIZE))\n",
        "words_in_pretrained = 0\n",
        "for i, word in enumerate(word2id.keys()):\n",
        "  try:\n",
        "    weights[i] = model[word]\n",
        "    words_in_pretrained += 1\n",
        "  except KeyError:\n",
        "    weights[i] = np.random.normal(scale=0.4, size=(EMB_SIZE,))\n",
        "weights = torch.from_numpy(weights.astype((np.float32)))\n",
        "\n",
        "print(f'学習済みベクトル利用単語数: {words_in_pretrained} / {VOCAB_SIZE}')\n",
        "print(weights.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdJmFWKNR-0E"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_size, padding_idx, output_size, hidden_size, num_layers, emb_weights=None, bidirectional=False, device=None):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.num_directions = bidirectional + 1 \n",
        "    if emb_weights != None: \n",
        "      self.emb = nn.Embedding.from_pretrained(emb_weights, padding_idx=padding_idx)\n",
        "    else:\n",
        "      self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
        "    self.rnn = nn.RNN(emb_size, hidden_size, num_layers, nonlinearity='tanh', bidirectional=bidirectional, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size * self.num_directions, output_size)\n",
        "    self.rnn_device = device\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.batch_size = x.size()[0]\n",
        "    hidden = self.init_hidden()\n",
        "    emb = self.emb(x)\n",
        "    out, hidden = self.rnn(emb, hidden)\n",
        "    out = self.fc(out[:, -1, :])\n",
        "    return out\n",
        "\n",
        "  def init_hidden(self):\n",
        "    hidden = torch.zeros(self.num_layers * self.num_directions, self.batch_size, self.hidden_size)\n",
        "    if (self.rnn_device != None):\n",
        "      hidden = hidden.to(self.rnn_device)\n",
        "    return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaraDGhNSDGD"
      },
      "source": [
        "# パラメータ\n",
        "VOCAB_SIZE = len(set(word2id.values())) + 1\n",
        "EMB_SIZE = 300\n",
        "PADDING_IDX = len(set(word2id.values()))\n",
        "OUTPUT_SIZE = 4\n",
        "HIDDEN_SIZE = 50\n",
        "NUM_LAYERS = 1\n",
        "LEARNING_RATE = 5e-2\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "device = torch.device('cuda')\n",
        "model = RNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, emb_weights=weights, device=device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "log = train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, collate_fn=Padsequence(PADDING_IDX), device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUFvZiBPSIG6"
      },
      "source": [
        "visualize_logs(log)\n",
        "\n",
        "_, acc_train = calculate_loss_and_accuracy(model, dataset_train, device)\n",
        "_, acc_test = calculate_loss_and_accuracy(model, dataset_test, device)\n",
        "print(f'正解率（学習データ）：{acc_train:.3f}')\n",
        "print(f'正解率（評価データ）：{acc_test:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvCNv-f3SL2V"
      },
      "source": [
        "## knock85"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf6azSfmSNT8"
      },
      "source": [
        "# パラメータ\n",
        "VOCAB_SIZE = len(set(word2id.values())) + 1\n",
        "EMB_SIZE = 300\n",
        "PADDING_IDX = len(set(word2id.values()))\n",
        "OUTPUT_SIZE = 4\n",
        "HIDDEN_SIZE = 50\n",
        "NUM_LAYERS = 2\n",
        "LEARNING_RATE = 5e-2\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "device = torch.device('cuda')\n",
        "model = RNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, emb_weights=weights, bidirectional=True, device=device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "log = train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, collate_fn=Padsequence(PADDING_IDX), device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "audi41U8SRKH"
      },
      "source": [
        "visualize_logs(log)\n",
        "\n",
        "_, acc_train = calculate_loss_and_accuracy(model, dataset_train, device)\n",
        "_, acc_test = calculate_loss_and_accuracy(model, dataset_test, device)\n",
        "print(f'正解率（学習データ）：{acc_train:.3f}')\n",
        "print(f'正解率（評価データ）：{acc_test:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Roz6pJtLSSau"
      },
      "source": [
        "## knock86"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjAuM-wPSUD9"
      },
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_size, padding_idx, output_size, out_channels, kernel_heights, stride, padding, emb_weights=None):\n",
        "    super().__init__()\n",
        "    if emb_weights != None:  \n",
        "      self.emb = nn.Embedding.from_pretrained(emb_weights, padding_idx=padding_idx)\n",
        "    else:\n",
        "      self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
        "    self.conv = nn.Conv2d(1, out_channels, (kernel_heights, emb_size), stride, (padding, 0))\n",
        "    self.drop = nn.Dropout(0.3)\n",
        "    self.fc = nn.Linear(out_channels, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    emb = self.emb(x).unsqueeze(1)\n",
        "    conv = self.conv(emb)\n",
        "    act = F.relu(conv.squeeze(3))\n",
        "    max_pool = F.max_pool1d(act, act.size()[2])\n",
        "    out = self.fc(self.drop(max_pool.squeeze(2)))\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPbTRYOCSYg1"
      },
      "source": [
        "# パラメータ\n",
        "VOCAB_SIZE = len(set(word2id.values())) + 1\n",
        "EMB_SIZE = 300\n",
        "PADDING_IDX = len(set(word2id.values()))\n",
        "OUTPUT_SIZE = 4\n",
        "OUT_CHANNELS = 100\n",
        "KERNEL_HEIGHTS = 3\n",
        "STRIDE = 1\n",
        "PADDING = 1\n",
        "\n",
        "model = CNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, OUT_CHANNELS, KERNEL_HEIGHTS, STRIDE, PADDING, emb_weights=weights)\n",
        "\n",
        "for i in range(10):\n",
        "  X = dataset_train[i]['inputs']\n",
        "  print(torch.softmax(model(X.unsqueeze(0)), dim=-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHyLo8dCSdZT"
      },
      "source": [
        "## knock87"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBnRqOZaSbQX"
      },
      "source": [
        "# パラメータ\n",
        "VOCAB_SIZE = len(set(word2id.values())) + 1\n",
        "EMB_SIZE = 300\n",
        "PADDING_IDX = len(set(word2id.values()))\n",
        "OUTPUT_SIZE = 4\n",
        "OUT_CHANNELS = 100\n",
        "KERNEL_HEIGHTS = 3\n",
        "STRIDE = 1\n",
        "PADDING = 1\n",
        "LEARNING_RATE = 5e-2\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "model = CNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, OUT_CHANNELS, KERNEL_HEIGHTS, STRIDE, PADDING, emb_weights=weights)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "device = torch.device('cuda')\n",
        "log = train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, collate_fn=Padsequence(PADDING_IDX), device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOhMng5vSjcf"
      },
      "source": [
        "visualize_logs(log)\n",
        "\n",
        "_, acc_train = calculate_loss_and_accuracy(model, dataset_train, device)\n",
        "_, acc_test = calculate_loss_and_accuracy(model, dataset_test, device)\n",
        "print(f'正解率（学習データ）：{acc_train:.3f}')\n",
        "print(f'正解率（評価データ）：{acc_test:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0_DkGnsTAn8"
      },
      "source": [
        "## knock88"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4WBfAFPTBy0"
      },
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "class textCNN(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_size, padding_idx, output_size, out_channels, conv_params, drop_rate, emb_weights=None):\n",
        "    super().__init__()\n",
        "    if emb_weights != None: \n",
        "      self.emb = nn.Embedding.from_pretrained(emb_weights, padding_idx=padding_idx)\n",
        "    else:\n",
        "      self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
        "    self.convs = nn.ModuleList([nn.Conv2d(1, out_channels, (kernel_height, emb_size), padding=(padding, 0)) for kernel_height, padding in conv_params])\n",
        "    self.drop = nn.Dropout(drop_rate)\n",
        "    self.fc = nn.Linear(len(conv_params) * out_channels, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    emb = self.emb(x).unsqueeze(1)\n",
        "    conv = [F.relu(conv(emb)).squeeze(3) for i, conv in enumerate(self.convs)]\n",
        "    max_pool = [F.max_pool1d(i, i.size(2)) for i in conv]\n",
        "    max_pool_cat = torch.cat(max_pool, 1)\n",
        "    out = self.fc(self.drop(max_pool_cat.squeeze(2)))\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1HpFd4cTK3X"
      },
      "source": [
        "!pip install optuna\n",
        "\n",
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "  emb_size = int(trial.suggest_discrete_uniform('emb_size', 100, 400, 100))\n",
        "  out_channels = int(trial.suggest_discrete_uniform('out_channels', 50, 200, 50))\n",
        "  drop_rate = trial.suggest_discrete_uniform('drop_rate', 0.0, 0.5, 0.1)\n",
        "  learning_rate = trial.suggest_loguniform('learning_rate', 5e-4, 5e-2)\n",
        "  momentum = trial.suggest_discrete_uniform('momentum', 0.5, 0.9, 0.1)\n",
        "  batch_size = int(trial.suggest_discrete_uniform('batch_size', 16, 128, 16))\n",
        "\n",
        "  VOCAB_SIZE = len(set(word2id.values())) + 1\n",
        "  PADDING_IDX = len(set(word2id.values()))\n",
        "  OUTPUT_SIZE = 4\n",
        "  CONV_PARAMS = [[2, 0], [3, 1], [4, 2]]\n",
        "  NUM_EPOCHS = 30\n",
        "\n",
        "  model = textCNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, out_channels, CONV_PARAMS, drop_rate, emb_weights=weights)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "  device = torch.device('cuda')\n",
        "  log = train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, NUM_EPOCHS, collate_fn=Padsequence(PADDING_IDX), device=device)\n",
        "  loss_valid, _ = calculate_loss_and_accuracy(model, dataset_valid, device, criterion=criterion) \n",
        "  return loss_valid "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaHcfm49TLNR"
      },
      "source": [
        "study = optuna.create_study()\n",
        "study.optimize(objective, timeout=7200)\n",
        "\n",
        "print('Best trial:')\n",
        "trial = study.best_trial\n",
        "print('  Value: {:.3f}'.format(trial.value))\n",
        "print('  Params: ')\n",
        "for key, value in trial.params.items():\n",
        "  print('    {}: {}'.format(key, value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0JjUzbLTdtI"
      },
      "source": [
        "# パラメータ\n",
        "VOCAB_SIZE = len(set(word2id.values())) + 1\n",
        "EMB_SIZE = int(trial.params['emb_size'])\n",
        "PADDING_IDX = len(set(word2id.values()))\n",
        "OUTPUT_SIZE = 4\n",
        "OUT_CHANNELS = int(trial.params['out_channels'])\n",
        "CONV_PARAMS = [[2, 0], [3, 1], [4, 2]]\n",
        "DROP_RATE = trial.params['drop_rate']\n",
        "LEARNING_RATE = trial.params['learning_rate']\n",
        "BATCH_SIZE = int(trial.params['batch_size'])\n",
        "NUM_EPOCHS = 30\n",
        "\n",
        "model = textCNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, OUT_CHANNELS, CONV_PARAMS, DROP_RATE, emb_weights=weights)\n",
        "print(model)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
        "device = torch.device('cuda')\n",
        "log = train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, collate_fn=Padsequence(PADDING_IDX), device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndvu9GDVTgA7"
      },
      "source": [
        "visualize_logs(log)\n",
        "\n",
        "_, acc_train = calculate_loss_and_accuracy(model, dataset_train, device)\n",
        "_, acc_test = calculate_loss_and_accuracy(model, dataset_test, device)\n",
        "print(f'正解率（学習データ）：{acc_train:.3f}')\n",
        "print(f'正解率（評価データ）：{acc_test:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axfgtSlRTGwo"
      },
      "source": [
        "## knock89"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4M94QUjTJiY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}