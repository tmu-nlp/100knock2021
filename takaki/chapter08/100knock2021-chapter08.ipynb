{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "100knock2021/chapter08.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMxDoDhlvvxmGwMQp+C6evu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/u7693/2c298faace8267f49923a58d9b7bc395/100knock2021-chapter08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K9dmi0fYd7u"
      },
      "source": [
        "# !wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
        "# !unzip NewsAggregatorDataset.zip\n",
        "# !sed -e 's/\"/'\\''/g' ./newsCorpora.csv > ./newsCorpora_re.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzsXLAeHYluy"
      },
      "source": [
        "## knock70"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raq_ca8-Ylly"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv('./newsCorpora_re.csv', header=None, sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n",
        "df = df.loc[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']), ['TITLE', 'CATEGORY']]\n",
        "\n",
        "train, valid_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=123, stratify=df['CATEGORY'])\n",
        "valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, random_state=123, stratify=valid_test['CATEGORY'])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMqtt8YtYtBa"
      },
      "source": [
        "# import gdown\n",
        "# \n",
        "# url = \"https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\"\n",
        "# output = 'GoogleNews-vectors-negative300.bin.gz'\n",
        "# gdown.download(url, output, quiet=True)\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eR7burhYvrO"
      },
      "source": [
        "import string\n",
        "import torch\n",
        "\n",
        "def transform_w2v(text):\n",
        "  table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "  words = text.translate(table).split() \n",
        "  vec = [model[word] for word in words if word in model]  \n",
        "\n",
        "  return torch.tensor(sum(vec) / len(vec))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBh35-sPYybJ"
      },
      "source": [
        "X_train = torch.stack([transform_w2v(text) for text in train['TITLE']])\n",
        "X_valid = torch.stack([transform_w2v(text) for text in valid['TITLE']])\n",
        "X_test = torch.stack([transform_w2v(text) for text in test['TITLE']])\n",
        "\n",
        "category_dict = {'b': 0, 't': 1, 'e':2, 'm':3}\n",
        "y_train = torch.tensor(train['CATEGORY'].map(lambda x: category_dict[x]).values)\n",
        "y_valid = torch.tensor(valid['CATEGORY'].map(lambda x: category_dict[x]).values)\n",
        "y_test = torch.tensor(test['CATEGORY'].map(lambda x: category_dict[x]).values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpCkW3meY27l"
      },
      "source": [
        "torch.save(X_train, 'X_train.pt')\n",
        "torch.save(X_valid, 'X_valid.pt')\n",
        "torch.save(X_test, 'X_test.pt')\n",
        "torch.save(y_train, 'y_train.pt')\n",
        "torch.save(y_valid, 'y_valid.pt')\n",
        "torch.save(y_test, 'y_test.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8J-5gWNY-J3"
      },
      "source": [
        "## knock71"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LE4zu9dHZAH0"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class SLPNet(nn.Module):\n",
        "  def __init__(self, input_size, output_size):\n",
        "    super().__init__()\n",
        "    self.fc = nn.Linear(input_size, output_size, bias=False)\n",
        "    nn.init.normal_(self.fc.weight, 0.0, 1.0)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwyY7x-XZC4P"
      },
      "source": [
        "model = SLPNet(300, 4) \n",
        "\n",
        "y_hat_1 = torch.softmax(model(X_train[:1]), dim=-1)\n",
        "Y_hat = torch.softmax(model.forward(X_train[:4]), dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKAhYU1JZPx3"
      },
      "source": [
        "## knock72"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfIyZgUZZRxf"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKS9SjJ0ZG2_"
      },
      "source": [
        "l_1 = criterion(model(X_train[:1]), y_train[:1])\n",
        "model.zero_grad()\n",
        "l_1.backward() \n",
        "print(f'損失: {l_1:.4f}')\n",
        "print(f'勾配:\\n{model.fc.weight.grad}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SowFn2h4ZMbb"
      },
      "source": [
        "l = criterion(model(X_train[:4]), y_train[:4])\n",
        "model.zero_grad()\n",
        "l.backward()\n",
        "print(f'損失: {l:.4f}')\n",
        "print(f'勾配:\\n{model.fc.weight.grad}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfUpHRUeZhkX"
      },
      "source": [
        "## knock73"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMjs2YUkZeRs"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "  def __init__(self, X, y):  \n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):  \n",
        "    return len(self.y)\n",
        "\n",
        "  def __getitem__(self, idx):  \n",
        "    return [self.X[idx], self.y[idx]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB8fKi0BZmjA"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset_train = NewsDataset(X_train, y_train)\n",
        "dataset_valid = NewsDataset(X_valid, y_valid)\n",
        "dataset_test = NewsDataset(X_test, y_test)\n",
        "\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=True)\n",
        "dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=len(dataset_test), shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6zhjLdeZog0"
      },
      "source": [
        "model = SLPNet(300, 4)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  loss_train = 0.0\n",
        "  for i, (inputs, labels) in enumerate(dataloader_train):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_train += loss.item()\n",
        "\n",
        "  loss_train = loss_train / i\n",
        "\n",
        "  model.eval() \n",
        "  with torch.no_grad():\n",
        "    inputs, labels = next(iter(dataloader_valid))\n",
        "    outputs = model(inputs)\n",
        "    loss_valid = criterion(outputs, labels)\n",
        "\n",
        "  print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, loss_valid: {loss_valid:.4f}')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytLVM-A6ZuM3"
      },
      "source": [
        "## knock74"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StYGDA7AZtTo"
      },
      "source": [
        "def calculate_accuracy(model, loader):\n",
        "  model.eval()\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in loader:\n",
        "      outputs = model(inputs)\n",
        "      pred = torch.argmax(outputs, dim=-1)\n",
        "      total += len(inputs)\n",
        "      correct += (pred == labels).sum().item()\n",
        "\n",
        "  return correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXt6tgCYZw98"
      },
      "source": [
        "acc_train = calculate_accuracy(model, dataloader_train)\n",
        "acc_test = calculate_accuracy(model, dataloader_test)\n",
        "print(f'正解率（学習データ）：{acc_train:.3f}')\n",
        "print(f'正解率（評価データ）：{acc_test:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMhU1YqRZyLi"
      },
      "source": [
        "## knock75"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnDmOxCSZzXA"
      },
      "source": [
        "def calculate_loss_and_accuracy(model, criterion, loader):\n",
        "  model.eval()\n",
        "  loss = 0.0\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in loader:\n",
        "      outputs = model(inputs)\n",
        "      loss += criterion(outputs, labels).item()\n",
        "      pred = torch.argmax(outputs, dim=-1)\n",
        "      total += len(inputs)\n",
        "      correct += (pred == labels).sum().item()\n",
        "\n",
        "  return loss / len(loader), correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCAQbH9CZ0jG"
      },
      "source": [
        "model = SLPNet(300, 4)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
        "\n",
        "num_epochs = 30\n",
        "log_train = []\n",
        "log_valid = []\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  for inputs, labels in dataloader_train:\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  loss_train, acc_train = calculate_loss_and_accuracy(model, criterion, dataloader_train)\n",
        "  loss_valid, acc_valid = calculate_loss_and_accuracy(model, criterion, dataloader_valid)\n",
        "  log_train.append([loss_train, acc_train])\n",
        "  log_valid.append([loss_valid, acc_valid])\n",
        "\n",
        "  print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua3WmTXzZ6fl"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "ax[0].plot(np.array(log_train).T[0], label='train')\n",
        "ax[0].plot(np.array(log_valid).T[0], label='valid')\n",
        "ax[0].set_xlabel('epoch')\n",
        "ax[0].set_ylabel('loss')\n",
        "ax[0].legend()\n",
        "ax[1].plot(np.array(log_train).T[1], label='train')\n",
        "ax[1].plot(np.array(log_valid).T[1], label='valid')\n",
        "ax[1].set_xlabel('epoch')\n",
        "ax[1].set_ylabel('accuracy')\n",
        "ax[1].legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTZv-CmYZ74-"
      },
      "source": [
        "## knock76"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeLTywAQZ9Qx"
      },
      "source": [
        "model = SLPNet(300, 4)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
        "\n",
        "num_epochs = 10\n",
        "log_train = []\n",
        "log_valid = []\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  for inputs, labels in dataloader_train:\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  loss_train, acc_train = calculate_loss_and_accuracy(model, criterion, dataloader_train)\n",
        "  loss_valid, acc_valid = calculate_loss_and_accuracy(model, criterion, dataloader_valid)\n",
        "  log_train.append([loss_train, acc_train])\n",
        "  log_valid.append([loss_valid, acc_valid])\n",
        "\n",
        "  torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, f'checkpoint{epoch + 1}.pt')\n",
        "\n",
        "  print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzQWOPF8aE3g"
      },
      "source": [
        "## knock77"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PC4khppkaDwy"
      },
      "source": [
        "import time\n",
        "\n",
        "def train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, num_epochs):\n",
        "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "  dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
        "\n",
        "  log_train = []\n",
        "  log_valid = []\n",
        "  for epoch in range(num_epochs):\n",
        "    s_time = time.time()\n",
        "\n",
        "    model.train()\n",
        "    for inputs, labels in dataloader_train:\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    loss_train, acc_train = calculate_loss_and_accuracy(model, criterion, dataloader_train)\n",
        "    loss_valid, acc_valid = calculate_loss_and_accuracy(model, criterion, dataloader_valid)\n",
        "    log_train.append([loss_train, acc_train])\n",
        "    log_valid.append([loss_valid, acc_valid])\n",
        "\n",
        "    torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, f'checkpoint{epoch + 1}.pt')\n",
        "\n",
        "    e_time = time.time()\n",
        "\n",
        "    print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}, {(e_time - s_time):.4f}sec') \n",
        "\n",
        "  return {'train': log_train, 'valid': log_valid}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu1MmrYQaKsW"
      },
      "source": [
        "dataset_train = NewsDataset(X_train, y_train)\n",
        "dataset_valid = NewsDataset(X_valid, y_valid)\n",
        "\n",
        "model = SLPNet(300, 4)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
        "\n",
        "for batch_size in [2 ** i for i in range(11)]:\n",
        "  print(f'batch size: {batch_size}')\n",
        "  log = train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyLTkG9gaSw6"
      },
      "source": [
        "## knock78"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN6cP8auaUN3"
      },
      "source": [
        "def calculate_loss_and_accuracy(model, criterion, loader, device):\n",
        "  model.eval()\n",
        "  loss = 0.0\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in loader:\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = model(inputs)\n",
        "      loss += criterion(outputs, labels).item()\n",
        "      pred = torch.argmax(outputs, dim=-1)\n",
        "      total += len(inputs)\n",
        "      correct += (pred == labels).sum().item()\n",
        "\n",
        "  return loss / len(loader), correct / total\n",
        "\n",
        "\n",
        "def train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, num_epochs, device=None):\n",
        "  model.to(device)\n",
        "\n",
        "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "  dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
        "\n",
        "  log_train = []\n",
        "  log_valid = []\n",
        "  for epoch in range(num_epochs):\n",
        "    s_time = time.time()\n",
        "\n",
        "    model.train()\n",
        "    for inputs, labels in dataloader_train:\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = model.forward(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    loss_train, acc_train = calculate_loss_and_accuracy(model, criterion, dataloader_train, device)\n",
        "    loss_valid, acc_valid = calculate_loss_and_accuracy(model, criterion, dataloader_valid, device)\n",
        "    log_train.append([loss_train, acc_train])\n",
        "    log_valid.append([loss_valid, acc_valid])\n",
        "\n",
        "    torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, f'checkpoint{epoch + 1}.pt')\n",
        "\n",
        "    e_time = time.time()\n",
        "\n",
        "    print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}, {(e_time - s_time):.4f}sec') \n",
        "\n",
        "  return {'train': log_train, 'valid': log_valid}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB-thilrabFC"
      },
      "source": [
        "dataset_train = CreateDataset(X_train, y_train)\n",
        "dataset_valid = CreateDataset(X_valid, y_valid)\n",
        "\n",
        "model = SLPNet(300, 4)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "for batch_size in [2 ** i for i in range(11)]:\n",
        "  print(f'バッチサイズ: {batch_size}')\n",
        "  log = train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, 1, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsI1ALVgafN_"
      },
      "source": [
        "## knock79"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu4ZN3C5ahfw"
      },
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "class MLPNet(nn.Module):\n",
        "  def __init__(self, input_size, mid_size, output_size, mid_layers):\n",
        "    super().__init__()\n",
        "    self.mid_layers = mid_layers\n",
        "    self.fc = nn.Linear(input_size, mid_size)\n",
        "    self.fc_mid = nn.Linear(mid_size, mid_size)\n",
        "    self.fc_out = nn.Linear(mid_size, output_size) \n",
        "    self.bn = nn.BatchNorm1d(mid_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc(x))\n",
        "    for _ in range(self.mid_layers):\n",
        "      x = F.relu(self.bn(self.fc_mid(x)))\n",
        "    x = F.relu(self.fc_out(x))\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlwArcsVaicg"
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "def calculate_loss_and_accuracy(model, criterion, loader, device):\n",
        "  model.eval()\n",
        "  loss = 0.0\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in loader:\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = model(inputs)\n",
        "      loss += criterion(outputs, labels).item()\n",
        "      pred = torch.argmax(outputs, dim=-1)\n",
        "      total += len(inputs)\n",
        "      correct += (pred == labels).sum().item()\n",
        "\n",
        "  return loss / len(loader), correct / total\n",
        "\n",
        "\n",
        "def train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, num_epochs, device=None):\n",
        "  model.to(device)\n",
        "\n",
        "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "  dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
        "\n",
        "  scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs, eta_min=1e-5, last_epoch=-1)\n",
        "\n",
        "  log_train = []\n",
        "  log_valid = []\n",
        "  for epoch in range(num_epochs):\n",
        "    s_time = time.time()\n",
        "\n",
        "    model.train()\n",
        "    for inputs, labels in dataloader_train:\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = model.forward(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    loss_train, acc_train = calculate_loss_and_accuracy(model, criterion, dataloader_train, device)\n",
        "    loss_valid, acc_valid = calculate_loss_and_accuracy(model, criterion, dataloader_valid, device)\n",
        "    log_train.append([loss_train, acc_train])\n",
        "    log_valid.append([loss_valid, acc_valid])\n",
        "\n",
        "    torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, f'checkpoint{epoch + 1}.pt')\n",
        "\n",
        "    e_time = time.time()\n",
        "\n",
        "    print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}, {(e_time - s_time):.4f}sec') \n",
        "\n",
        "    if epoch > 2 and log_valid[epoch - 3][0] <= log_valid[epoch - 2][0] <= log_valid[epoch - 1][0] <= log_valid[epoch][0]:\n",
        "      break\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "  return {'train': log_train, 'valid': log_valid}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRtqLM8Eap0p"
      },
      "source": [
        "dataset_train = CreateDataset(X_train, y_train)\n",
        "dataset_valid = CreateDataset(X_valid, y_valid)\n",
        "\n",
        "model = MLPNet(300, 200, 4, 1)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "log = train_model(dataset_train, dataset_valid, 64, model, criterion, optimizer, 1000, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeBV4re-atNS"
      },
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "ax[0].plot(np.array(log['train']).T[0], label='train')\n",
        "ax[0].plot(np.array(log['valid']).T[0], label='valid')\n",
        "ax[0].set_xlabel('epoch')\n",
        "ax[0].set_ylabel('loss')\n",
        "ax[0].legend()\n",
        "ax[1].plot(np.array(log['train']).T[1], label='train')\n",
        "ax[1].plot(np.array(log['valid']).T[1], label='valid')\n",
        "ax[1].set_xlabel('epoch')\n",
        "ax[1].set_ylabel('accuracy')\n",
        "ax[1].legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_Iy2g7latuo"
      },
      "source": [
        "def calculate_accuracy(model, loader, device):\n",
        "  model.eval()\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in loader:\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = model(inputs)\n",
        "      pred = torch.argmax(outputs, dim=-1)\n",
        "      total += len(inputs)\n",
        "      correct += (pred == labels).sum().item()\n",
        "\n",
        "  return correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GULSNajWavb0"
      },
      "source": [
        "acc_train = calculate_accuracy(model, dataloader_train, device)\n",
        "acc_test = calculate_accuracy(model, dataloader_test, device)\n",
        "print(f'正解率（学習データ）：{acc_train:.3f}')\n",
        "print(f'正解率（評価データ）：{acc_test:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}