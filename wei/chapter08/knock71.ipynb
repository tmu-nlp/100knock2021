{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "knock71.ipynb",
      "provenance": [],
      "history_visible": true,
      "mount_file_id": "1V6nbqyHr_0mkXf3TuZBYVuLNSCICtceb",
      "authorship_tag": "ABX9TyPATJiob+RmI6iq2VFXyxQ3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tmu-nlp/100knock2021/blob/main/wei/chapter08/knock71.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwXrktkv9ro3"
      },
      "source": [
        "## Task Description   \n",
        "71. 単層ニューラルネットワークによる予測\n",
        "\n",
        "問題70で保存した行列を読み込み，学習データについて以下の計算を実行せよ．\n",
        "$$\\hat{y}_1=softmax(x_1W),\\\\\\hat{Y}=softmax(X_{[1:4]}W)$$\n",
        "$X_{[1:4]}∈\\mathbb{R}^{4×d}$は特徴ベクトル$x_1$,$x_2$,$x_3$,$x_4$を縦に並べた行列である．\n",
        "* 行列$W \\in \\mathbb{R}^{d \\times L}$は単層ニューラルネットワークの重み行列で，ここではランダムな値で初期化\n",
        "* $\\hat{\\boldsymbol y_1} \\in \\mathbb{R}^L$は未学習の行列$W$で事例$x_1$を分類したときに，各カテゴリに属する確率を表すベクトル\n",
        "* 学習事例集合の場合では、各カテゴリに属する確率を行列$\\hat{Y} \\in \\mathbb{R}^{n \\times L}$として表現\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_eulk7dx9T3"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# データの読込\n",
        "df = pd.read_csv('drive/MyDrive/ColabNotebooks/NLPknock100/newsCorpora_re.csv', header=None, sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n",
        "\n",
        "# データの抽出\n",
        "df = df.loc[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']), ['TITLE', 'CATEGORY']]\n",
        "\n",
        "# データの分割\n",
        "train, valid_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=123, stratify=df['CATEGORY'])\n",
        "valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, random_state=123, stratify=valid_test['CATEGORY'])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqgRucY5F-wD"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "import torch\n",
        "import string\n",
        "\n",
        "# 学習済み単語ベクトルを読み込む\n",
        "model = KeyedVectors.load_word2vec_format('drive/MyDrive/ColabNotebooks/NLPknock100/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "\n",
        "def transform_w2v(text):\n",
        "  table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "  words = text.translate(table).split()  # 記号をスペースに置換後、スペースで分割してリスト化\n",
        "  vec = [model[word] for word in words if word in model]  # 1語ずつベクトル化\n",
        "\n",
        "  return torch.tensor(sum(vec) / len(vec))  # 平均ベクトルをTensor型に変換して出力\n",
        "\n",
        "X_train = torch.stack([transform_w2v(text) for text in train['TITLE']])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptxVuZvouqWJ"
      },
      "source": [
        "# SGLNetという単層ニューラルネットワークを定義\n",
        "from torch import nn\n",
        "\n",
        "class SGLNet(nn.Module):\n",
        "  #　ネットのlayerを定義\n",
        "  def __init__(self, input_size, output_size):\n",
        "    super().__init__()\n",
        "    self.fc = nn.Linear(input_size, output_size, bias=False)\n",
        "    nn.init.normal_(self.fc.weight, 0.0, 1.0)  # 正規乱数で重みを初期化\n",
        "  #　forwardで入力データが順伝播時に通るレイヤーを順に配置しておく\n",
        "  def forward(self, x):\n",
        "    x = self.fc(x)\n",
        "    return x\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOfZum57wiZl",
        "outputId": "87aaf99a-6cad-4465-80da-36eb8737b9c9"
      },
      "source": [
        "# 単層ニューラルネットワークの初期化\n",
        "SigelNNmodel = SGLNet(300, 4) \n",
        "#未学習の行列Wで事例x_1を分類したとき，各カテゴリに属する確率を表すベクトル\n",
        "y_hat_1 = torch.softmax(SigelNNmodel(X_train[:1]), dim=-1)\n",
        "print(y_hat_1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.2993, 0.0426, 0.4832, 0.1749]], grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kr3XaPdhCluQ",
        "outputId": "d1ff826a-61c8-4a1c-b5fe-f1e83a017362"
      },
      "source": [
        "Y_hat = torch.softmax(SigelNNmodel.forward(X_train[:4]), dim=-1)\n",
        "print(Y_hat)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.2993, 0.0426, 0.4832, 0.1749],\n",
            "        [0.2223, 0.6417, 0.1280, 0.0080],\n",
            "        [0.1510, 0.6303, 0.1328, 0.0859],\n",
            "        [0.1233, 0.3885, 0.1048, 0.3834]], grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}