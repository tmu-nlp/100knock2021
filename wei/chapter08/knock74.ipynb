{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "knock74.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "16LIr3nl01kd4HE3c6S-osDiXcOQ4i77G",
      "authorship_tag": "ABX9TyOMeWXKwg3ITCFvMRLLN34B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tmu-nlp/100knock2021/blob/main/wei/chapter08/knock74.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXLnX3qOVE-r"
      },
      "source": [
        "## Task Description\n",
        "\n",
        "74．正解率の計測\n",
        "  \n",
        "問題73で求めた行列を用いて学習データおよび評価データの事例を分類したとき，その正解率をそれぞれ求めよ．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdaxgdFZYsL-"
      },
      "source": [
        "##正解率を算出する関数を定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAPS0o3cHrru"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTs0fkv7XTrt"
      },
      "source": [
        "# 学習した単層モデルとDataloaderを入力として、正解率を算出する関数を定義\n",
        "import torch\n",
        "def calculate_accuracy(model, loader):\n",
        "  model.eval()\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in loader:\n",
        "      outputs = model(inputs)\n",
        "      pred = torch.argmax(outputs, dim=-1)\n",
        "      total += len(inputs)\n",
        "      correct += (pred == labels).sum().item()\n",
        "\n",
        "  return correct / total"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqqv6BPsYa-m"
      },
      "source": [
        "## Dataloaderを準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_eulk7dx9T3"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# データの読込\n",
        "df = pd.read_csv('drive/MyDrive/ColabNotebooks/NLPknock100/newsCorpora_re.csv', header=None, sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n",
        "\n",
        "# データの抽出\n",
        "df = df.loc[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']), ['TITLE', 'CATEGORY']]\n",
        "\n",
        "# データの分割\n",
        "train, valid_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=123, stratify=df['CATEGORY'])\n",
        "valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, random_state=123, stratify=valid_test['CATEGORY'])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0WTh1OhF6rV"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "import string\n",
        "import torch\n",
        "# 学習済み単語ベクトルを読み込む\n",
        "model = KeyedVectors.load_word2vec_format('drive/MyDrive/ColabNotebooks/NLPknock100/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "\n",
        "def transform_w2v(text):\n",
        "  table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "  words = text.translate(table).split()  # 記号をスペースに置換後、スペースで分割してリスト化\n",
        "  vec = [model[word] for word in words if word in model]  # 1語ずつベクトル化\n",
        "\n",
        "  return torch.tensor(sum(vec) / len(vec))  # 平均ベクトルをTensor型に変換して出力\n",
        "\n",
        "X_train = torch.stack([transform_w2v(text) for text in train['TITLE']])\n",
        "X_valid = torch.stack([transform_w2v(text) for text in valid['TITLE']])\n",
        "X_test = torch.stack([transform_w2v(text) for text in test['TITLE']])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUGAmqumGWT4"
      },
      "source": [
        "# ラベルベクトルの作成\n",
        "category_dict = {'b': 0, 't': 1, 'e':2, 'm':3}\n",
        "y_train = torch.tensor(train['CATEGORY'].map(lambda x: category_dict[x]).values)\n",
        "y_valid = torch.tensor(valid['CATEGORY'].map(lambda x: category_dict[x]).values)\n",
        "y_test = torch.tensor(test['CATEGORY'].map(lambda x: category_dict[x]).values)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzYhiLzYKttL"
      },
      "source": [
        "# 学習用のTensor型の平均化ベクトルとラベルベクトルを変換\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "  def __init__(self, X, y):  # datasetの構成要素を指定\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):  # len(dataset)で返す値を指定\n",
        "    return len(self.y)\n",
        "\n",
        "  def __getitem__(self, idx):  # dataset[idx]で返す値を指定\n",
        "    return [self.X[idx], self.y[idx]]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hh2Gx-ihP_WZ"
      },
      "source": [
        "#　Datasetを作成するには、X_train, y_trainを準備\n",
        "dataset_train = NewsDataset(X_train, y_train)\n",
        "dataset_valid = NewsDataset(X_valid, y_valid)\n",
        "dataset_test = NewsDataset(X_test, y_test)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUyOBKbXQ30c"
      },
      "source": [
        "# Dataloaderの作成\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=True)\n",
        "dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=len(dataset_test), shuffle=False)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3-_J4G5ZG03"
      },
      "source": [
        "## 単層NNモデルを作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksWFiXGiG6iU"
      },
      "source": [
        "# SGLNetという単層ニューラルネットワークを定義\n",
        "from torch import nn\n",
        "\n",
        "class SGLNet(nn.Module):\n",
        "  #　ネットのlayerを定義\n",
        "  def __init__(self, input_size, output_size):\n",
        "    super().__init__()\n",
        "    self.fc = nn.Linear(input_size, output_size, bias=False)\n",
        "    nn.init.normal_(self.fc.weight, 0.0, 1.0)  # 正規乱数で重みを初期化\n",
        "  #　forwardで入力データが順伝播時に通るレイヤーを順に配置しておく\n",
        "  def forward(self, x):\n",
        "    x = self.fc(x)\n",
        "    return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1csqfe3XLbq"
      },
      "source": [
        "# モデルの定義\n",
        "SigleNNmodel = SGLNet(300, 4)\n",
        "\n",
        "# 損失関数の定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# オプティマイザの定義\n",
        "optimizer = torch.optim.SGD(SigleNNmodel.parameters(), lr=1e-1)\n",
        "\n",
        "# 学習\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "  # 訓練モードに設定\n",
        "  SigleNNmodel.train()\n",
        "  loss_train = 0.0\n",
        "  for i, (inputs, labels) in enumerate(dataloader_train):\n",
        "    # 勾配をゼロで初期化\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 順伝播 + 誤差逆伝播 + 重み更新\n",
        "    outputs = SigleNNmodel(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Mk8-qWEZfb9"
      },
      "source": [
        "## 出力"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERMpAfDWZ0pC",
        "outputId": "0a8b0732-593e-4671-ce12-2cd32ee878bb"
      },
      "source": [
        "acc_train = calculate_accuracy(SigleNNmodel, dataloader_train)\n",
        "acc_test = calculate_accuracy(SigleNNmodel, dataloader_test)\n",
        "print(f'正解率（学習データ）：{acc_train:.3f}')\n",
        "print(f'正解率（評価データ）：{acc_test:.3f}')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "正解率（学習データ）：0.923\n",
            "正解率（評価データ）：0.902\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}